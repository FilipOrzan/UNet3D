{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpngMOV08J_B",
        "outputId": "f3afa149-616c-4266-d3f1-daf188f689ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qjcRIkU8M5V"
      },
      "outputs": [],
      "source": [
        "!pip install torchio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAgRLYD9Tvlg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import torchio as tio\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from scipy.ndimage import rotate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#Reproductibilitate\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "# Configurare globalƒÉ\n",
        "CONFIG = {\n",
        "    \"seed\": 42,\n",
        "    \"batch_size\": 24,\n",
        "    \"epochs\": 20,\n",
        "    \"learning_rate\": 1.67e-5,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])\n",
        "\n",
        "device = torch.device(CONFIG[\"device\"])\n",
        "torch.use_deterministic_algorithms(False)\n",
        "\n",
        "\n",
        "# Definim Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        targets = targets.float()  # Conversie la FloatTensor\n",
        "        bce_loss = F.binary_cross_entropy(preds, targets, reduction=\"none\")\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
        "\n",
        "# Dice Loss\n",
        "class WeightedDiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, smooth=1e-6):\n",
        "        super(WeightedDiceLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.contiguous().view(-1)\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        intersection = (pred * target).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (\n",
        "            pred.sum() + target.sum() + self.smooth\n",
        "        )\n",
        "\n",
        "        if self.weight is not None:\n",
        "            # Aplica ponderi reale: 1 - dice = loss, »ôi √Æl ponderƒÉm\n",
        "            weighted_dice_loss = self.weight[1] * (1 - dice) + self.weight[0] * dice\n",
        "            return weighted_dice_loss\n",
        "        else:\n",
        "            return 1 - dice\n",
        "\n",
        "\n",
        "# Combined Loss (Focal + Dice)\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, weight=None, ce_weight=0.5, dice_weight=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.ce = nn.BCELoss()  # Sau po»õi folosi nn.BCEWithLogitsLoss() dacƒÉ nu ai sigmoid √Æn model\n",
        "        self.dice = WeightedDiceLoss(weight=weight)\n",
        "        self.ce_weight = ce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        ce_loss = self.ce(preds, targets.float())\n",
        "        dice_loss = self.dice(preds, targets)\n",
        "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss\n",
        "\n",
        "# Dice score pentru validare\n",
        "def dice_score(preds, targets, threshold=0.5):\n",
        "    preds = (preds > threshold).float()\n",
        "    intersection = (preds * targets).sum()\n",
        "    union = preds.sum() + targets.sum()\n",
        "    return (2. * intersection) / (union + 1e-8)\n",
        "\n",
        "# Dataset wrapper pentru lista de patch-uri\n",
        "class PatchDataset(Dataset):\n",
        "    def __init__(self, patches):\n",
        "        self.patches = patches\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, mask = self.patches[idx]\n",
        "        return torch.tensor(img).unsqueeze(0).float(), torch.tensor(mask).unsqueeze(0).float()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AttentionBlock3D(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock3D, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv3d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm3d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv3d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm3d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm3d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "class UNet3D_Attention(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super(UNet3D_Attention, self).__init__()\n",
        "\n",
        "        def CBR(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm3d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout3d(p=dropout_rate),\n",
        "                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm3d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout3d(p=dropout_rate)\n",
        "            )\n",
        "\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.enc1 = CBR(1, 32)\n",
        "        self.enc2 = CBR(32, 64)\n",
        "        self.enc3 = CBR(64, 128)\n",
        "        self.enc4 = CBR(128, 256)\n",
        "\n",
        "        self.bottleneck = CBR(256, 512)\n",
        "\n",
        "        # Attention gates\n",
        "        self.att4 = AttentionBlock3D(F_g=512, F_l=256, F_int=128)\n",
        "        self.att3 = AttentionBlock3D(F_g=256, F_l=128, F_int=64)\n",
        "        self.att2 = AttentionBlock3D(F_g=128, F_l=64, F_int=32)\n",
        "        self.att1 = AttentionBlock3D(F_g=64, F_l=32, F_int=16)\n",
        "\n",
        "        # Decoder blocks\n",
        "        self.dec4 = CBR(512 + 256, 256)\n",
        "        self.dec3 = CBR(256 + 128, 128)\n",
        "        self.dec2 = CBR(128 + 64, 64)\n",
        "        self.dec1 = CBR(64 + 32, 32)\n",
        "\n",
        "        self.final = nn.Conv3d(32, 1, kernel_size=1)\n",
        "\n",
        "    def center_crop(self, enc_feat, target_size):\n",
        "        _, _, d, h, w = enc_feat.size()\n",
        "        td, th, tw = target_size\n",
        "        d1 = (d - td) // 2\n",
        "        h1 = (h - th) // 2\n",
        "        w1 = (w - tw) // 2\n",
        "        return enc_feat[:, :, d1:d1+td, h1:h1+th, w1:w1+tw]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        # Decoder + Attention\n",
        "        up4 = F.interpolate(bottleneck, size=enc4.shape[2:], mode='trilinear', align_corners=True)\n",
        "        att4 = self.att4(g=up4, x=self.center_crop(enc4, up4.shape[2:]))\n",
        "        dec4 = self.dec4(torch.cat([up4, att4], dim=1))\n",
        "\n",
        "        up3 = F.interpolate(dec4, size=enc3.shape[2:], mode='trilinear', align_corners=True)\n",
        "        att3 = self.att3(g=up3, x=self.center_crop(enc3, up3.shape[2:]))\n",
        "        dec3 = self.dec3(torch.cat([up3, att3], dim=1))\n",
        "\n",
        "        up2 = F.interpolate(dec3, size=enc2.shape[2:], mode='trilinear', align_corners=True)\n",
        "        att2 = self.att2(g=up2, x=self.center_crop(enc2, up2.shape[2:]))\n",
        "        dec2 = self.dec2(torch.cat([up2, att2], dim=1))\n",
        "\n",
        "        up1 = F.interpolate(dec2, size=enc1.shape[2:], mode='trilinear', align_corners=True)\n",
        "        att1 = self.att1(g=up1, x=self.center_crop(enc1, up1.shape[2:]))\n",
        "        dec1 = self.dec1(torch.cat([up1, att1], dim=1))\n",
        "\n",
        "        output = self.final(dec1)\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "# Func»õie de antrenare\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    dice_scores = []\n",
        "\n",
        "    for imgs, masks in loader:\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(imgs)\n",
        "        loss = criterion(output, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dice_scores.append(dice_score(output, masks).item())\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    avg_dice = np.mean(dice_scores)\n",
        "    return avg_loss, avg_dice\n",
        "\n",
        "# Func»õie de validare\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    dice_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in loader:\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            dice_scores.append(dice_score(outputs, masks).item())\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    avg_dice = np.mean(dice_scores)\n",
        "    return avg_loss, avg_dice\n",
        "\n",
        "# 5-Fold Cross-Validation cu Early Stopping\n",
        "def run_kfold_cv(patch_dataset, num_folds=5, num_epochs=50, batch_size=2, lr=1e-4, patience=10, device='cuda'):\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "    all_fold_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(patch_dataset)):\n",
        "        print(f\"\\nüîÅ Fold {fold+1}/{num_folds}\")\n",
        "        train_set = Subset(patch_dataset, train_idx)\n",
        "        val_set = Subset(patch_dataset, val_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=1, shuffle=False)\n",
        "\n",
        "        model = UNet3D_Attention().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        best_dice = 0.0\n",
        "        patience_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss, train_dice = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "            val_loss, val_dice = validate(model, val_loader, criterion, device)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: \"\n",
        "                  f\"Train Loss = {train_loss:.4f}, Train Dice = {train_dice:.4f} | \"\n",
        "                  f\"Val Loss = {val_loss:.4f}, Val Dice = {val_dice:.4f}\")\n",
        "\n",
        "            if val_dice > best_dice:\n",
        "                best_dice = val_dice\n",
        "                patience_counter = 0\n",
        "                best_model_state = model.state_dict()\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"‚èπÔ∏è Early stopping triggered at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        all_fold_scores.append(best_dice)\n",
        "        torch.save(best_model_state, f\"/content/drive/MyDrive/best_model_fold_{fold+1}.pt\")\n",
        "        print(f\"‚úÖ Fold {fold+1} Best Dice: {best_dice:.4f}\")\n",
        "\n",
        "    avg_dice = np.mean(all_fold_scores)\n",
        "    print(f\"\\nüìä Final Dice Score AVG over {num_folds} folds: {avg_dice:.4f}\")\n",
        "    return all_fold_scores\n",
        "\n",
        "\n",
        "dataset = PatchDataset(all_patches)\n",
        "fold_scores = run_kfold_cv(dataset, num_folds=5, num_epochs=50, batch_size=2, patience=10, device='cuda')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}